## Sentence generation

This package provides some methods to use GPT-based generation, word2vec embeddings, and NLP pipelines to generate a corpus of sentences then cross-references frequency lists to generate an ordered corpus of sentences. The corpus is then sampled by a Gaussian distribution with the mean positioned at the targeted difficulty level, with respect to the frequency list.

The sample is then converted to a similarity matrix between all pairs of sentences and saved as a HDF5 file. The corpus is re-sampled to prevent this matrix from becoming too large. The matrix is loaded into the front-end.

**Note [Aug 29, 2024]:**

* Front-end build is not updated to load the h5 matrix yet.
* Planning to add English and Arabic corpora

### Requirements

This module uses:
* `spacy` to tokenize.
* `gensim` to load word2vec embeddings.
* `openai` to access the OpenAI API.
* `h5py` and `sqlite3` to generate output files.

### Instructions for re-producing or adding to corpora

Use `populate.py` to add to the corpus, either remove or increase the `max_merge_size` argument, then enable the desired generation methods (`from_vocab`, `from_freq`, `from_context`, `from_sentence`). These are options to add from a custom vocab list, a frequency list, or prompt GPT to generate sentences based on parts of speech. The last option exists to generate small variations to existing sentences that must have already been generated by the previous steps.

Current setups include using [OpenAI API](https://platform.openai.com/docs/overview) or [Ollama](https://ollama.com/). The Llama3.1-8B model can produce sentences but with a fairly high error rate. `fixgen.py` methods can clear some of the improperly generated sentences. `receive_batch.py` and `send_batch.py` batch the existing corpus into the OpenAI API for another layer of proofreading.

You can download pre-trained word embeddings from:
1. https://fasttext.cc/docs/en/crawl-vectors.html
2. https://wikipedia2vec.github.io/wikipedia2vec/pretrained/

Either format (*.vec or *_300d.txt) can be loaded by `gensim`. These are large files and not included in the repository

[spaCy pipelines](https://spacy.io/models) should be downloaded into your environment.

e.g., `python -m spacy download en_core_web_sm`

Frequency lists can be downloaded from https://wortschatz.uni-leipzig.de/en/download. Download then extract the `*_words.txt` file. Some are already included in the repository.

#### Hierarchy

```
├───receive_batch.py
├───send_batch.py
├───populate.py
├───utils # You shouldn't need to change much in here
│   ├───cleanfreqlist.py
│   └─── ...
├───settings
│   ├───config.py # You must add your own OpenAI API Key
│   ├───embeddings.py # Update embedding file names here
│   ├───freqlists.py # Update frequency list file names here
│   ├───pipelines.py # Update spaCy pipeline names here
│   └─── ...
├───data
│   ├───gen # Generated corpora saved here
│   │   ├───merges # Consolidated merges of generated corpora
│   │   │   ├───de
│   │   │   │   ├───sentences.json # Unordered merge
│   │   │   │   ├───sentences_sorted.json # vs Freq List
│   │   │   │   └───sentences_sorted_pruned.json # After downsampling
│   │   │   └───...
│   │   └───sentences # New generations saved here
│   ├───simmtx # Output similiarity matrices will be saved here
│   │   ├───de # Organized into ISO code subdirectories
│   │   │   ├───*.h5 # Saves multiple samples
│   │   │   └─── ...
│   │   └─── ...
│   ├───vocab # Place your downloaded frequency lists here
│   │   ├───de
│   │   │   ├───*.txt
│   │   │   └───*.json # Cleaned up version
│   │   └─── ...
│   └───word2vec # Place your downloaded word embeddings here
│       ├───*.vec
│       └───*.txt
└───batches # for handling OpenAI batch data
```

Languages are indexed following [ISO 639](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) standards. 

Recent merges of corpora are included in the repository as JSON in `data/gen/merges`.

Pre-generated .h5 similarity matrices (sample size = 10000 sentences from the corpus) for a few languages have been saved here:

| Language | Corpus Source Size |
| ----------- | ----------- |
| Chinese (zh) | [15657](https://drive.google.com/file/d/1Oooau49p_kWNdJHU11RduTOU___9ZUHo/view?usp=drive_link) |
| French (fr) | [12823](https://drive.google.com/file/d/1NknolVQEqXdmP2dvF81ZkOI-4Zt0P-Uj/view?usp=drive_link) | 
| German (de) | [12156](https://drive.google.com/file/d/1whmH9u6Sejil4_F24WJU8iwB9t-Tlabk/view?usp=drive_link) | 
| Japanese (ja) | [94098](https://drive.google.com/file/d/10C1N6Apmy43wn_G5DJiksIPW9lEX0y-v/view?usp=drive_link) | 
| Korean (ko) | [12292](https://drive.google.com/file/d/11RCPM1ag_j88L-IgTldea4c7EJqkn5pd/view?usp=drive_link) | 
| Russian (ru) | [12492](https://drive.google.com/file/d/192meoj3qUndZxKvTiUhsvPK-NE0zz92u/view?usp=drive_link) | 
| Spanish (es) | [31402](https://drive.google.com/file/d/1kJrWUQLyXqSqSpEsjUNNAZXtkjzYdC92/view?usp=drive_link) | 

**Note [Aug 29, 2024]:** 

* Batch-level GPT generation to be added later

### Current coverage of corpora

Sentences are tokenized via spacy and each token is cross-referenced to a frequency list. The following statistics are referenced against mostly the 10K Wikipedia dumps from https://wortschatz.uni-leipzig.de/en/download. Frequency corpuses are trimmed of punctuation and entities. See `cleanfreqlist.py`.

Smaller corpora tend to have lower coverage. This will get better as the corpora are built up. Frequency lists are also of different size. 

| Language | Corpus size | Corpus coverage over frequency list | Wiki |
| ----------- | ----------- | ----------- | ----------- |
| Chinese (zh) | 15657 | 46.42% | [2021 Wikipedia 10K dump](https://wortschatz.uni-leipzig.de/en/download/Chinese) |
| French (fr) | 19817 | 78.96% | [2021 Wikipedia 10K dump](https://wortschatz.uni-leipzig.de/en/download/French) |
| German (de) | 12156 | 78.40% | [2021 Wikipedia 10K dump](https://wortschatz.uni-leipzig.de/en/download/German) |
| Japanese (ja) | 94098 | 88.41% | [2021 Wikipedia 10K dump](https://wortschatz.uni-leipzig.de/en/download/Japanese) | 
| Korean (ko) | 12292 | 55.68% | [2021 Wikipedia 10K dump](https://wortschatz.uni-leipzig.de/en/download/Korean) |
| Russian (ru) | 12492 | 68.57% | [2021 Wikipedia 10K dump](https://wortschatz.uni-leipzig.de/en/download/Russian) |
| Spanish (es) | 31402 | 81.45% | [2021 Wikipedia 10K dump](https://wortschatz.uni-leipzig.de/en/download/Spanish) |


